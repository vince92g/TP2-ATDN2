# -*- coding: utf-8 -*-
"""TP2 ATDN2 VG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGFcJHFP4BI2Yq51gVfFocdB0bqiJHfR
"""

# Importation des bibliothèques nécessaires à la réalisation du tp
!pip install scikit-learn
import numpy as np
import pandas as pd
!pip install scikit-optimize
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
import matplotlib.pyplot as plt
from sklearn.kernel_ridge import KernelRidge
from sklearn.metrics import accuracy_score
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.svm import SVC
from sklearn.kernel_ridge import KernelRidge

# Lecture des données
data = pd.read_csv('tp2_atdn_donnees.csv')
data.head()
data.columns
# On étudie le rendement agricole en fonction de l'humidité et de la température
X = data[['Humidité (%)', 'Température (°C)']]
y = data['Rendement agricole (t/ha)']
# Séparation des données de test et d'entrainement
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Espace sur lequel nous travaillons
space  = [Integer(10, 100, name='n_estimators'),
          Real(0.01, 1.0, "uniform", name='min_samples_split'),  # Changed to (0.01, 1.0) and "uniform"
          Integer(2, 10, name='max_depth'),
          Integer(1, 10, name='min_samples_leaf')]


# Fonction objectif à minimiser (erreur quadratique moyenne)
@use_named_args(space)
def objective(**params):
    model = RandomForestRegressor(**params, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    return rmse


# Exécution de l'optimisation bayésienne
res_gp = gp_minimize(objective, space, n_calls=50, random_state=42)


# Visualisation des étapes du processus
plt.plot(res_gp.func_vals)
plt.xlabel("Nombre d'itérations")
plt.ylabel("RMSE")
plt.title("Optimisation Bayésienne")
plt.show()


# Comparaison avec Grid Search et Random Search
# Grid Search
param_grid = {'n_estimators': [20, 50, 100], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}
grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5)
grid_search.fit(X_train, y_train)
grid_rmse = np.sqrt(mean_squared_error(y_test, grid_search.predict(X_test)))

# Random Search
param_dist = {'n_estimators': np.arange(10, 101), 'max_depth': [None, 10, 20, 30], 'min_samples_split': np.arange(2, 11), 'min_samples_leaf': np.arange(1, 5)}
random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_distributions=param_dist, n_iter=50, cv=5, random_state=42)
random_search.fit(X_train, y_train)
random_rmse = np.sqrt(mean_squared_error(y_test, random_search.predict(X_test)))

print("RMSE optimisation bayésienne:", res_gp.fun)
print("RMSE Grid Search:", grid_rmse)
print("RMSE Random Search:", random_rmse)

# Meilleurs hyperparamètres de l'optimisation bayésienne
print("Meilleurs hyperparamètres (optimisation bayésienne):", res_gp.x)

# lecture des données
data = pd.read_csv('tp2_atdn_donnees.csv')
X = data[['Humidité (%)', 'Température (°C)']]
y = data['Rendement agricole (t/ha)']
# séparation des données en données de test et d'entrainement
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# régression bayésienne à noyau
model = KernelRidge(alpha=1.0, kernel='rbf', gamma=0.1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
# calcul du rmse
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE: {rmse}")

# On prend toute les valeurs obtenues pour chaque variable
X_plot = np.linspace((X_test['Humidité (%)'].min(), X_test['Température (°C)'].min()),
                     (X_test['Humidité (%)'].max(), X_test['Température (°C)'].max()),
                     100)
X_plot = pd.DataFrame(X_plot, columns=['Humidité (%)', 'Température (°C)'])
lower_bound = y_pred - 0.5
upper_bound = y_pred + 0.5

# tableau de la régression bayésienne avec prédiction et intervalle de confiance
plt.scatter(X_test['Humidité (%)'], y_test, color='blue', label='Actuel')
plt.plot(X_plot['Humidité (%)'], model.predict(X_plot), color='red', label='Predictions')
plt.fill_between(X_plot['Humidité (%)'].ravel(), lower_bound, upper_bound, color='gray', alpha=0.3, label='Intervalle de confiance')
plt.xlabel('Humidité (%)')
plt.ylabel('Rendement agricole (t/ha)')
plt.title('Régression ridge du noyau bayésien')
plt.legend()
plt.show()

data = pd.read_csv('tp2_atdn_donnees.csv')

# On prédit le type de sol en fonction des données climatiques (température et humidité)
X = data[['Humidité (%)', 'Température (°C)']]
y = data['Type de sol']

# Séparation des données en données de test et d'entrainement
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Classification du noyau gaussien
kernel = 1.0 * RBF(1.0)
gpc_model = GaussianProcessClassifier(kernel=kernel, random_state=42)
gpc_model.fit(X_train, y_train)
gpc_predictions = gpc_model.predict(X_test)
gpc_accuracy = accuracy_score(y_test, gpc_predictions)

# SVM classique
svm_model = SVC(kernel='rbf', random_state=42)
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_predictions)

# Comparaison des résultats
print("Précision de la classification du noyau bayésien:", gpc_accuracy)
print("Précision du SVM classique:", svm_accuracy)